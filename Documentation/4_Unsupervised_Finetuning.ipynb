{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b0c47e-9cb2-49d5-95f3-a8dd37c89e58",
   "metadata": {},
   "source": [
    "# Unsupervised Finetuning\n",
    "The following notebook describes the Unsupervised Finetuning-process.\n",
    "\n",
    "## Start Training\n",
    "\n",
    "\n",
    "To start the training, we have to restart the textgeneration-webui, using the following commands:\n",
    "    \n",
    "    conda activate textgen\n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/Llama-2-13b-chat-hf --load-in-8bit\n",
    "\n",
    "Thereby, we load the selected base model (Llama-2-13b-chat-hf) in 8bit as recommended by the text-generation-webui-documentation.\n",
    "    \n",
    "After the webui started, the following steps have been executed within the webui:\n",
    "1. Switch to tab \"Training\"\n",
    "2. Give the LoRA-file a new name - in our case \"Pre_ant_adapter\"\n",
    "3. Adapt the Hyperparameters for the training:\n",
    "    -\tTraining-epochs: 3\n",
    "    -\tLearning-rate: 3e^-4\n",
    "    -\tLoRA Rank: 8\n",
    "    -\tLoRA Alpha: 16\n",
    "    -\tBatch Size: 128\n",
    "    -\tMicro Batch Size: 4\n",
    "    -\tCutoff Length: 256\n",
    "    -\tLR Scheduler: linear\n",
    "    -\tOverlap Length: 128\n",
    "    -\tPrefer Newline Cut Length: 128\n",
    "\n",
    "4. Go to the Tab \"Raw text file\"\n",
    "5. Select under \"Text file\" the folder \"Unstructured_training_data\"\n",
    "6. Click \"Start LoRA Training\"\n",
    "\n",
    "After 6 hours the new LoRA-adapter was saved to the folder \"text-generation-webui/loras/Pre_ant_adapter\".\n",
    "\n",
    "### Create Pre-ant-model\n",
    "We decided to create the new Pre-ant-model by merging the newly trained LORA-adapter (Pre-ant-adapter) with the corresponding base model (Llama-2-13b-chat-hf). Therefore, we executed the following code from this notebook:\n",
    "\n",
    "#### Load Base-model, tokenizer, LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715ac1a-785a-49a7-b12e-70b17ad5cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b6aa63-cec5-4b8d-b137-de6891cfdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a22d6-fe22-4ac7-a861-e4923c76406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"text-generation-webui/models/Llama-2-13b-chat-hf/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732bf942-50f3-40c1-9024-19593eb43e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7cff6-6f48-40bf-8c05-f1efe58184f8",
   "metadata": {},
   "source": [
    "#### Combine Base-model with LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7c8479-2bc8-4540-821c-ba82879d692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, 'text-generation-webui/loras/Pre-ant-adapter/', is_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6ba8d-12b1-47f7-b287-99956475376b",
   "metadata": {},
   "source": [
    "#### Merge Base-model with LoRA-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b20cfe-e2eb-4e1b-8cdc-fdbaee7e55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6e3bc-a1ba-4d84-be3e-237453f88aec",
   "metadata": {},
   "source": [
    "#### Save merged model to text-generation-webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450159f-c854-4c9f-85c0-66d6ef5a7e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"text-generation-webui/models/Pre-ant-model/\", safe_serializetion=True)\n",
    "tokenizer.save_pretrained(\"text-generation-webui/models/Pre-ant-model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - Cuda (ipykernel)",
   "language": "python",
   "name": "python3-jr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
