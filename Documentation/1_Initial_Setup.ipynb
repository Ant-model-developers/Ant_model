{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41269570-7964-491c-8312-c1d0b8b4faa5",
   "metadata": {},
   "source": [
    "# Structure of the repository\n",
    "\n",
    "In this repository, you can find three folders: Documentation, Text-generation-webui and Chatbot-interface. \n",
    "- Documentation-Folder: Within this folder you can find the documentation of the different steps conducted during the implementation and evaluation of the artefact.\n",
    "- text-generation-webui: This folder provides the environment for the fine-tuning- and the perplexity- and loss-evaluation-steps.\n",
    "- Chatbot-interface: This folder provides the chatbot-environment were the qualitative evaluation was conducted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401fd3c-f0db-4aa3-9437-d1cc90ce3aac",
   "metadata": {},
   "source": [
    "## Text-generation-webui\n",
    "\n",
    "The Text-generation-webui-folder is based on the text-generation-webui developed by Oobabooga: [https://github.com/oobabooga/text-generation-webui]. The following parts describe the initial setup of a conda-environment for the text-generation-webui, the download of the text-generation-webui, the necessary adaptions of the underlying files and how the text-generation-webui can be started.\n",
    "\n",
    "### Setup Conda-environment\n",
    "The following commands have been executed within the terminal of jupyter-lab to setup a conda-environment for the text-generation-webui:\n",
    "    \n",
    "    conda create -n textgen python=3.10.9 -y\n",
    "    conda activate textgen\n",
    "    \n",
    "### Download text-generation-webui\n",
    "The following commands have been executed within the terminal of jupyter-lab to download and setup the text-generation-webui:\n",
    "\n",
    "    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "    git clone https://github.com/oobabooga/text-generation-webui\n",
    "    cd text-generation-webui\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dfb40-f60a-4d79-bc86-9c6d48584013",
   "metadata": {},
   "source": [
    "### Adaptions of the text-generation-webui [Can be skipped as soon as the webui works again correctly]\n",
    "At the time of development, there was a bug in the text-generation-webui. If you tried to train a model, the following error message occured: UnboundLocalError: local variable 'tokens' referenced before assignment\n",
    "\n",
    "We fixed this bug manually by adapting the tokenization_llama.py file in the transformers-package. The following code have been used to access the file via the terminal:\n",
    "\n",
    "    cd /home/[...]/.conda/envs/textgen/lib/python3.10/site-packages/transformers/models/llama/\n",
    "    edit tokenization_llama.py\n",
    "    \n",
    "The following adaptions have been made to the file:\n",
    "\n",
    "Original function:\n",
    "\n",
    "\n",
    "    def tokenize(self, text: \"TextInput\", **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless \n",
    "        the first token is special.\n",
    "        \"\"\"\n",
    "        if self.legacy:\n",
    "            return super().tokenize(text, **kwargs)\n",
    "\n",
    "        if len(tokens)>1: \n",
    "            tokens = super().tokenize(SPIECE_UNDERLINE + text.replace(SPIECE_UNDERLINE, \" \"), **kwargs)\n",
    "\n",
    "        if tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n",
    "            tokens = tokens[1:]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "Adapted function:\n",
    "\n",
    "    def tokenize(self, text: \"TextInput\", **kwargs) -> List[str]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless \n",
    "        the first token is special.\n",
    "        \"\"\"\n",
    "        if self.legacy:\n",
    "            return super().tokenize(text, **kwargs)\n",
    "\n",
    "        tokens = super().tokenize(SPIECE_UNDERLINE + text.replace(SPIECE_UNDERLINE, \" \"), **kwargs)\n",
    "\n",
    "        if len(tokens)>1 and tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n",
    "            tokens = tokens[1:]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba7863-2017-4b03-b341-0290d3d85b8c",
   "metadata": {},
   "source": [
    "### Starting the Webui\n",
    "The text-generation-webui can be startet via the terminal using the following commands:\n",
    "    \n",
    "    cd /home/[...]/text-generation-webui/\n",
    "    python server.py --share --model models/Llama-2-7b-chat-hf\n",
    "    \n",
    "The textgeneration-webui runs on a local and public url. Copy the public url into your browser to access the webui."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef91804-5e58-4685-990b-358b9c545ff8",
   "metadata": {},
   "source": [
    "## Chatbot-interface\n",
    "To initialize the artefact in a chatbot interface, we firstly setup a new conda-environment. Therefore, we used the following commands in the terminal:\n",
    "\n",
    "    conda create -n evaluation python=3.10.9 -y\n",
    "    conda activate evaluation\n",
    "    pip install transformers gradio torch accelerate\n",
    "    \n",
    "Afterwards, we created a gradio-app [https://www.gradio.app/] in the file \"Gradio_app.py\". The gradio-app can be started using the following commands in the terminal:\n",
    "\n",
    "    cd /home/[...]/Chatbot-interface/\n",
    "    python Gradio_app.py\n",
    "    \n",
    "Attention: If you want to use this gradio app, you have to enter your huggingface-token in line 6 in the document \"Gradio_app.py\".   \n",
    "    \n",
    "The gradio app runs on a local and public url. Copy the public url into your browser to access the webui. Furthermore, you can find in the sub-folder \"Example_processes\" one process example used in the qualitative evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07abebf-573b-4c23-a044-3044f8420de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - Cuda (ipykernel)",
   "language": "python",
   "name": "python3-jr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
